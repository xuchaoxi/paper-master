\chapter{总结与展望}

\section{总结}
    本文围绕即席视频检索这个十分重要且很有挑战的问题，本文通过调研了目前主流的视频检索算法，总结了这些算法都有的三个关键组件，即文本表示、视频表示和公共空间表示。本文基于深度学习技术，对原本用于图文检索的W2VV算法的文本表示和公共空间进行表示改进，提出了适用于即席视频检索任务上的W2VV++深度学习算法，并且不同于以往基于概念的方法，W2VV++是不依赖与概念建模的，并且可以对模型进行端到端的训练，因此可以联合优化文本表示和在公共空间中的跨模态相似度计算。在W2VV++算法的基础上，本文继续探究文本特征表示的融合策略，提出了多空间学习的SEA算法，本文通过科学系统的评测，验证了这两个算法在视频检索上具有很大的优越性。本文的研究成果可以概括如下：
    \begin{enumerate}[\hspace{2em}1.]
        \item 本文首先对即席视频检索领域的权威评测TRECVID上的前三名的算法围绕三个关键组件进行了综述：即从文本表示、视频表示和公共空间表示这三个在跨模态检索上的关键方面总结了这些算法的优势和不足。然后本文从这三个方面还列举并分析了目前主流的基于文本的视频检索算法。
        \item 本文分析了原本用于图文检索的W2VV算法的不足，并针对这些不足进行了改进，即使用更好的目标函数ITRL监督模型训练和使用GRU输出的所有隐藏层状态的平均作为文本表示的一部分，并适用在即席视频检索任务上，通过系统的实验证明了这两个改进对算法性能的提升都起到了关键作用。
        \item 本文继续研究的多空间学习的SEA算法相对W2VV++的效果更进一步，证明了通过多空间学习的方式融合不同的文本编码器的效果比直接拼接的方式更好，而且SEA算法在MSR-VTT和TRECVID两个数据上都取得了目前最好的视频检索的性能。

    \end{enumerate}


\section{展望}
    本文的研究内容仍然有很多可以继续研究并完善的地方，本文在这里列举，希望对读者有所启发：

    \begin{enumerate}[\hspace{2em}1.]
        \item 本文的实验数据的视频时长都比较短，因此我们通过简单地对视频帧特征进行平均池化的操作来获得视频级的特征。然而这种简单的方法会导致视频的时序信息的丢失，对更长的视频中的时空信息的深入挖掘对视频的检索和理解具有重要的研究意义。

        \item 本文使用的文本编码器bag-of-words，word2vec和基于循环神经网络的GRU都不是现在最前沿的文本表示方法，探究更好的文本编码器，如目前在多项自然语言处理任务上取得突破成果的BERT模型，在跨模态的视频检索上的性能也具有重要的研究价值。

        \item 本文所使用的目标函数ITRL中只考虑了文本与视频的相似度关系，实际上文本与文本的相似性、视频之间的相似性对基于文本的视频检索效果都会有影响，因此如何将这些相似性联系起来一起作为优化目标是值得研究的问题。

    \end{enumerate}
