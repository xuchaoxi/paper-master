\chapter{研究综述}

\section{即席视频检索}

本文回顾了 2016 年至 2019 年四年的 TRECVID 评测中的即席视频检索系统（Ad-hoc Video Search，AVS）~\cite{awad2016trecvid,awad2017trecvid,awad2018trecvid,awad2019trecvid}任务的效果较好的算法，因为 TRECVID 评测是最有挑战的基准，吸引了该领域包括卡耐基梅隆大学、弗吉尼亚大学、香港中文大学等学术界和阿里巴巴等工业界的一些优秀学者参与评测并且提出了很多有效的算法~\cite{}。该评测的测试数据的正确答案在参与者提交评测结果前是不公开的，并且参与者不允许为任何的查询样例调整他们的结果，这样可以公平地评价参与者的模型的泛化能力。
表格\ref{tab:method-diffs}从三个关键组成部分分析这些算法的异同，即自然语言查询是如何表示的（查询表示）、未标注的视频是如何表示的（视频表示）、在什么特征空间进行跨模态匹配（公共空间）。

\input{chap/table-related}

在 TRECVID 2016 评测中，Le等人在文献~\cite{le2016nii}中提出一种基于文本查询的解决算法。他们先用预
训练的卷积神经网络（VGG-16）对视频中的帧图像提取语义概念特征，包括主要的物体、场景属性、
物体之间的关系等。在获取视频的语义概念后，视频查询任务就转化成了文本查询任务，即以查询的文本检索视频的概念，
他们使用标准的 TF-IDF 技术来计算每个概念特征的权重。给定的查询与视频的相似度由查询
文本与视频的概念特征在文本空间中计算得出。
Markatopoulou等人~\cite{foteini2016iti}提出了一种类似的算
法，即先通过预训练的卷积神经网络（AlexNet， GoogLeNet， ResNet 和 VGGNet）来提取视
频关键帧的 1000 维 ImageNet~\cite{russakovsky2015imagenet}的概念特征和 345 维 TRECVID SIN~\cite{smeaton2009high}的概念特征，把两
类特征做拼接并且将通过这四个深度卷积网络得到的特征取平均来表示视频， 每一个概念
的维度上的数值表示该概念在视频中出现的概率。在文本表示上，他们通过显式语义分析（ESA~\cite{gabrilovich2007computing}）的方
式来计算查询文本与视频的 1345 个概念之间的相关性，如果这个相关性大于一个给定的阈值，则
选择这个概念来表示这个查询文本。不同于前两种算法，Liang 等人~\cite{liang2016inf}使用 webly-labeled
learning~\cite{liang2016learning}算法来对每个查询进行建模，即先通过word2vec的稠密向量进行表示，而视频则使用VGG-19和C3D提取视觉特征。
为了解决零样本的问题，他们利用查询文本从Youtube 上爬取了弱标注的视频数据来训练模型，尽管这种方法被证明有效果~\cite{kordumova2015best}，但是对于一些复杂的查询来说，从网上自动爬取相关的视频还是很困难的。因此在实际的评测中，这种方法不如前两种方法有效。

在 TRECVID 2017 评测中，尽管基于概念的查询和特征表示仍然很主流，获得评测第
一名的 Snoek 等人在文献\cite{snoek2017university}中提出一种更加优雅的表示方式——VideoStory~\cite{habibian2014videostory}。对于每个没有标注的视频，他们利用深度卷积神经网络提取深度特征，并通过线性变换将该特征转换到
VideoStory 空间进行表示。这个表示进一步通过线性变换转换为词袋向量（Bag-of-words，BoW），而每个查
询也用词袋向量进行表示。因此，视频与查询间的余弦相似度可以在词袋向量空间直接进行
计算。尽管这种方法效果很好，但是也存在两个明显的不足：第一，词袋向量（BoW）
忽略了查询语句的时序信息；第二，词袋向量的有效性依赖于合适的词典选择，但这是无法
和表示学习一同进行优化的，即一旦确定了词典，则所有文本的表示都是固定的。
相反，本文提出的通过深度神经网络来表示查询既考虑了词语
的重要性，也考虑了词语间的时序信息，并且可以端到端地进行训练优化文本查询的表示。
而第二名的Ueki等人~\cite{ueki2017waseda}使用Alexnet和GoogLeNet分别在ImageNet~\cite{deng2009imagenet}、Places~\cite{zhou2014learning}、FCVID~\cite{jiang2017exploiting}、UCF101~\cite{soomro2012ucf101}等
数据集上收集了包括物体、场景、动作等一共5万维的概念向量组成了一个很大的概念库，而Nguyen等人~\cite{nguyen2017vireo}使用了类似的方案，他们使用ResNet-50提取了2774维的概念向量，评测结果排在第三名，由此可以合适的概念选择对检索的效果影响很大。

在 TRECVID 2018 评测中，许多基于深度学习的方法来表示查询的方案出现了。
Huang等人~\cite{huang2018informedia}使用了两种方法，第一种方法是基于概念的方法，即先使用预训练的深度卷积神经网络
在YFCC~\cite{thomee2016yfcc100m}，ImageNet Shuffle~\cite{mettes2016the}，UCF101~\cite{soomro2012ucf101}，Kinetics~\cite{carreira2017quo}，Places~\cite{zhou2014learning}，Google Sports~\cite{karpathy2014large}等数据上提取了一共13,626维的概念向量，而文本表示则通过基于规则的概念选择得到文本的概念向量表示，最终文本查询与候选视频的相似度在概念向量空间计算得到。第二种方法是通过联合嵌入空间的方法为文本学习连续的向量表示，即先用预训练的卷积神经网络Inception-v4~\cite{szegedy2016inception}提取视频的视觉特征，然后他们叠加了两个注意力模型Dual Attention Network~\cite{nam2017dual}和Stacked Cross-Attention Network~\cite{lee2018stacked}，学习一个子空间来计算跨模态的相似度。最终，他们后融合这两种方法的结果。
而Bastan等人~\cite{bastan2018ntu}使用 VSE++~\cite{faghri2017vse++}模型，即先使用预训练的ResNet-152提取视频的视觉特征，使用Gated Recurrent Unit（GRU）~\cite{cho2014learning}编码查询文本，然后为两个模态表示学习一个公共的子空间，该方法获得了评测的第三名。而本文研究的基于深度学习的方法W2VV++取得评测的第一名。

在 TRECVID 2019 评测中，排名前三的方法都是基于深度学习的。
排在第一名的Wu等人\cite{wu2019hybrid}使用了混合的序列编码策略对视频特征和文本的词向量进行聚集建模。由于查询句子和视频都是序列的模态，即视频由一系列的图像组成，句子由一系列的单词组成，因此视频深度特征通常是先对视频提取帧图像，然后再对图像使用卷积神经网络ResNet-152到的是一系列的图像特征，同样句子先经过word2vec模型得到一系列的单词的词向量。为了得到最终的视频表示和句子表示，他们使用平均池化、GRU~\cite{cho2014learning}、NetVLAD~\cite{arandjelovic2016netvlad}和图卷积网络~\cite{mao2018hierarchical}的方式对这些序列的特征进行聚集表达成视频向量和句子向量，然后通过拼接的方式相对应地融合这些特征来作视频和句子的最终向量表示，最后他们为这两个模态的向量联学习一个公共的子空间。

\section{跨模态匹配的特征融合}
即席视频检索本质上是一个以文本搜视频的跨模态匹配问题，因此本文也关注到一些基于深度学习解决文本-视频的跨模态匹配任务的工作。
因为文本-视频的跨模态匹配的核心是计算文本与视频的相关度，而文本与视频具有的异构性的特点，因此需要把它们投到异构公共的空间计算，而在这之前则需要分别对文本和视频进行向量化表示。
视频是由一序列的帧图像和一段音频组成，既包括了静态的对象也含有动态的信息，是很复杂的多媒体数据，因此对视频进行准确地向量表示也是困难的。
Ueki等人~\cite{ueki2019waseda}为了充分提取视频的图像信息，使用了ResNet-50，ResNet-101和ResNet-152三种深度卷积神经网络提取视频的帧图像视觉特征，并且为每个特征单独地学习一个文本-视频联合的公共子空间，最后使用平均后融合的方式融合这些子空间从而得到最终的视频检索结果。虽然他们使用不同的深度卷积神经网络提取视频的特征，但这几种神经网络提取的特征是很相似的，它们的网络结构是类似的，只是网络的层数逐渐更多更深，因此提取到的特征只是表达效果越来越好，但特征间缺乏互补性。考虑到视频的音频信息和动态信息，Mithun等人~\cite{mithun2018learning}除了使用ResNet-152提出视频的帧图像特征作为视频静态的物体特征，他们还使用I3D~\cite{carreira2017quo}对视频提取活动的特征和SoundNet CNN~\cite{aytar2016soundnet}来提取音频信息，然后它们使用拼接的方式融合这两种动态信息作为视频动态的活动特征，它们为物体特征和活动特征分别单独地学习公共子空间，分别在每个空间计算对应的文本与视频的相似度，最终平均这两个相似度作为文本-视频对的相似度，并根据这个相似度对候选视频排序，它们的模型结构图如图~\ref{fig:related_fig1}所示。

\begin{figure*}[tbh!]
    \centering
    \includegraphics[width=\linewidth]{figures/related_fig1}
    \caption[Mithun等人的活动-文本空间和物体-文本空间模型结构图]{\textbf{活动-文本空间和物体-文本空间模型结构图}，来自~\cite{mithun2018learning}。}
    \label{fig:related_fig1}
\end{figure*}

而在视频片段检索领域，Yu等人~\cite{vsrel2020}使用了类似的做法，他们首先使用预训练的卷积神经网络VGG提取视频的帧图像特征，并使用平均池化的操作得到视频的物体特征，另外，他们还使用一个双流的卷积神经网络temporal segment network（TSN）~\cite{wang2016temporal}来提取视频的活动特征，该双流网络同时以帧图像和视频的光流作为输入。同样，他们单独地为这两类特征学习一个公共空间，并使用平均的方式后融合这两个公共空间。

考虑到视频复杂信息，Liu等人~\cite{liu2019use}尽可能地挖掘了视频信息，他们提取了视频的物体特征、场景特征、动作特征、人脸特征、字符特征（optical character recognition，OCR）、言语（speech）特征和音频（audio）特征。他们首先使用深度卷积神经网络SENet-154~\cite{}模型提取视频帧图像的物体特征，在Places365~\cite{}数据上预训练的DenseNet-161~\cite{}提取帧图像的场景特征，双流卷积神经网络I3D提取连续帧图像的动作特征。而对于人脸特征，他们先使用人脸目标检测网络SSD~\cite{}来提取帧图像中的人脸，然后再使用在VGGFace2~\cite{}数据上预训练的ResNet-50提取每个检测出的人脸的特征。对于OCR特征，他们先使用文本检测模型PixelLink~\cite{}来定位图像中的文本位置，然后每个文本框内的图像区域经过一个在文本识别数据Synth90K~\cite{}的模型~\cite{}得到一序列字符串的文本，最后文本的每个单词的向量由预训练的word2vec模型提取。对于言语特征，他们使用谷歌云的API来对声音提取单词，然后得到的句子的单词的向量同样由word2vec模型提取。对于音频特征，他们使用在Youtube-8m~\cite{}上预训练的audio CNN~\cite{}提取。最后，他们使用平均池化的方法来处理这些帧图像的物体特征、场景特征、动作特征和人脸特征，而使用NetVLAD的方法聚集OCR特征、言语特征和音频特征，从而最终得到视频级的相应的特征表示。为了融合所有的这些视频级特征，他们首先将这些特征做了线性投影，将所有特征转换到相同维度的特征，然后计算两两特征间的关系，然后根据这个关系对转换后的特征进行缩放处理得到新的特征表示，


在最近的由 Dong 等人\cite{dong2019dual}提出的对偶编码在即席视频检索上取得了很好的效果,他们
通过使用词袋模型(Bag-of-words)编码句子的全局信息,Gated Recurrent Unit(GRU)~\cite{cho2014learning}编码句
子的时序信息和卷积网络(CNN)编码句子的局部信息,但他们只是将这三部分信息进行简单
的拼接,并不能最优地利用这些信息的互补性,本文将研究通过构建多子空间的方式来充分
有效地利用这些信息。


\section{本章小结}
