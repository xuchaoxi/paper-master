\chapter{研究综述}

\section{即席视频检索}

本文回顾了 2016 年至 2019 年四年的 TRECVID 评测中的即席视频检索系统(AVS)~\cite{awad2016trecvid,awad2017trecvid,awad2018trecvid,awad2019trecvid}任务的效果较好的算法,因为 TRECVID 评测是最有挑战的基准,吸引了该领域包括卡耐基梅隆大学、弗吉尼亚大学、香港中文大学等学术界和阿里巴巴等工业界的一些重要学者参与评测并且提出了很多有效的算法。表\ref{tab:method-diffs}从三个关键组成部分分析这些算法的异同,即自然语言查询是如何表示的(查询表示)、未标注的视频是如何表示的(视频表示)、在什么特征空间进行跨模态匹配(公共空间)。

\input{chap/table-related}

在 TRECVID 2016 评测中,Le \etal\cite{le2016nii}提出一种基于文本查询的解决算法。他们先用预
训练的卷积神经网络(VGG-16)对视频中的帧提取语义概念特征,包括主要的物体、场景属性、
物体之间的关系、元数据。获取视频的语义概念后,视频查询任务就相当于文本查询任务,
他们使用标准的 TF-IDF 技术来计算每个概念特征的权重。给定查询与视频的相似度由查询
文本与视频的概念特征在文本空间中计算得出。Markatopoulou \etal\cite{foteini2016iti}提出了一种类似的算
法,即先通过预训练的卷积神经网络(AlexNet, GoogLeNet, ResNet 和 VGGNet)来提取视
频关键帧的 1000 维 ImageNet~\cite{russakovsky2015imagenet}的概念特征和 345 维 TRECVID SIN~\cite{smeaton2009high}的概念特征,把两
类特征做拼接并且将通过这四个深度卷积网络得到的特征取平均来表示视频, 每一个概念
的维度上的数值表示该概念在视频中出现的概率。但他们通过显式语义分析(ESA~\cite{gabrilovich2007computing})的方
式来计算查询文本与 1345 个概念之前的相关性,如果这个相关性大于一个给定的阈值,则
选择这个概念来表示这个查询文本。不同于前两种算法,Liang \etal\cite{liang2016inf}使用 webly-labeled
learning~\cite{liang2016learning}算法来对每个查询进行建模,为了解决零样本的问题,他们利用查询文本从
Youtube 上爬取了弱标注的视频数据来训练模型,尽管这种方法被证明有效果~\cite{kordumova2015best},但是对于
一些复杂的查询来说,从网上自动爬取相关的视频还是很困难的。实际上,在 TRECVID 2016
评测中,这种方法不如前两种方法有效。

在 TRECVID 2017 评测中,尽管基于概念的查询和特征表示仍然很主流,获得该评测第
一名的 Snoek \etal\cite{snoek2017university}提出一种更加优雅的表示方式——VideoStory\\~\cite{habibian2014videostory}。对于每个没有标注
的视频,他们利用卷积神经网络提取深度特征,并通过线性变换将该特征转换到所谓的
VideoStory 表示。这个表示进一步通过线性变换转换为词袋向量(Bag-of-words),而每个查
询也用词袋向量进行表示。因此,视频与查询间的余弦相似度可以在词袋向量空间直接进行
计算。尽管这种方法效果很好,但是也存在两个明显的不足:第一,词袋向量(Bag-of-words)
忽略了查询语句的时序信息;第二,词袋向量的有效性依赖于合适的词语选择,但这是无法
和表示学习一同进行优化的。相反,本文提出的通过深度神经网络来表示查询既考虑了词语
的重要性,也考虑了词语间的时序信息,并且可以端到端地进行训练。

在 TRECVID 2018 评测中,许多基于深度学习的方法来表示查询的方案出现了。Huang \etal\cite{huang2018informedia}使用了两个注意力模型和传统的基于概念的表示方法。Bastan \etal\cite{bastan2018ntu}使用 VSE++~\cite{faghri2017vse++}
模型,获得了评测的第三名。而本文研究的基于全深度学习的方法 W2VV++效果最好。

在 TRECVID 2019 评测中,Wu \etal\cite{wu2019hybrid}使用了混合的序列编码策略(GRU\\~\cite{cho2014learning},
NetVLAD~\cite{arandjelovic2016netvlad}, DCGN~\cite{mao2018hierarchical})对视频的帧特征和文本的词特征进行建模,通过特征拼接的方式
得到更加鲁棒的视频表示和文本表示,获得了评测的第一名,虽然混合编码策略的效果得到
了验证,但是把三种特征进行简单的拼接并不是最优的选择。

在最近的由 Dong \etal\cite{dong2019dual}提出的对偶编码在即席视频检索上取得了很好的效果,他们
通过使用词袋模型(Bag-of-words)编码句子的全局信息,Gated Recurrent Unit(GRU)~\cite{cho2014learning}编码句
子的时序信息和卷积网络(CNN)编码句子的局部信息,但他们只是将这三部分信息进行简单
的拼接,并不能最优地利用这些信息的互补性,本文将研究通过构建多子空间的方式来充分
有效地利用这些信息。
