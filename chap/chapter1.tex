\chapter{绪论}
%\section{安装\LaTeX{} }
%\subsection{Mac OS X}
%\begin{figure}[htbp]
%\centering\includegraphics[width=5cm,height=1.32cm]{figures/Logo_2.pdf}
%\caption[示意图]{用LaTeX画图}
%\end{figure}

\section{研究背景与意义}
随着移动互联网的快速发展，人们可以随时随地通过智能设备上的应用创建和分享各种不同模态的多媒体数据，如文本、图像、视频等。其中视频
数据具有更丰富的信息，受到了更多用户的青睐，据《第44次中国互联网络发展状况统计报告》~\footnote{http://www.cac.gov.cn/2019-08/30/c\_1124938750.htm}显示，截至2019年6月，我国的网络视频用户规模
达7.59亿，占网民整体的88.8\%，其中短视频用户规模为6.48亿，占网民整体的75.8\%。各种短视频的智能设备应用也是呈井喷式的发展，如抖音、
快手等应用，用户可以通过这些应用创建和分享一分钟以内的短视频，据《2019年抖音数据报告》~\footnote{https://weibo.com/ttarticle/p/show?id=2309404457716281114897}显示，截至2020年1月5日，抖音日活跃用户数已经超过4亿，而根据快手发布的《2019快手内容报告》~\footnote{http://www.chinanews.com/business/2020/02-22/9100846.shtml}
显示，2019年有2.5亿用户通过快手平台发布作品，平台内有近200亿条的海量视频数据。在如此庞大的并且还在快速增长的视频数据里，
如何高效和精确地检索出用户需要或者感兴趣的视频是一个具有很大挑战并且具有应用价值的问题，是多媒体检索领域内的热点问题~\cite{hong2017,geetha2008a,hu2011a,peng2018an}。

即席视频检索检索(Ad-hoc Video Search, AVS)是指用户根据自己的需求以句子的形式来查询未经标注的视频,例如“一个有胡子的男人对着麦克风讲话或唱歌”~\cite{awad2016trecvid}。这和经典的基于内容的视频检索~\cite{yu2015content}不同，基于内容的视频检索是以一个图像或者视频片段
作为查询，从候选的视频数据中检索出与该查询内容相同或者相似的视频。这种视频检索方式在实际应用中有很大的局限性，因为用户通常不具有
能够表达其查询需求的图片或者视频，而以句子的形式表达用户需求则是一种更加直接方便的方式，例如用户希望检索出“一个男人在唱歌”这样场景的视频，而其并没有这样的样例视频，则其不能通过这种基于视频内容的视频检索技术进行检索，因此用户会更加倾向于使用句子的形式作为查询来检索想要的视频。即席视频检索也不同于长期研究的基于概念的视频检索技术~\cite{snoekcees2009concept}，这种检索技术的和核心是从视频中
检测出某些特定的概念，例如视频中出现的人或者物体，然后对查询句子提取关键词，利用查询关键词与检测出的概念进行匹配，这种检索技术需要
先定义一个概念集，然后检测视频中出现的概念，因此检索效果受限于概念的选择并且这种技术对查询句子的语义也没有进行建模分析。而本文研究
的即席视频检索是解决用户以一种模态的多媒体对象（文本）作为查询输入，检索出与查询语义上相同或相近的另一种模态的多媒体对象（视频），
这本质是也是一直被广泛关注的跨模态检索技术~\cite{rasiwasia2010a,feng2014cross,pereira2014on,suris2018cross,mithun2018learning}。
即席视频检索的核心是计算查询文本与候选视频在语义上的相关度，然后根据该相关度对候选视频进行排序，从而从候选视频中选出用户期望的视频，图~\ref{}展示了即席视频检索的大致流程。由于查询文本与视频是两种不同的模态，它们在底层表示上是异构的，即文本是由一序列的单词排列组成，而视频是由一序列图像组成并且伴随着音频，这也通常被称为“异构鸿沟”，因此这两种数据是不能直接进行比较或者计算相关度的。这两种
媒体数据的异构性和不可比性，使得即席视频检索是一个非常具有挑战性的研究任务。得益于美国国家标准与技术研究院（National Institute of Standards and Technology， NIST）举行的视频检索国际权威评测TRECVID AVS，提供的大量的测试数据集合、统一的评价标准，吸引了来自包括卡内基梅隆大学、弗吉尼亚大学、香港中文大学、早稻田大学等全球各地的优秀学者参与评测，推动了即席视频检索技术的发展~\cite{awad2016trecvid,awad2017trecvid,awad2018trecvid,awad2019trecvid}。如表格\ref{tab:method-diffs}所示，最近四年的研究方法都是基于公共空间学习的方法，这种方法为文本和视频学习一个公共空间，并将这两种模态的数据投影到公共空间进行表示，使得这两种数据的相关度可以在这个公共空间通过直接计算距离进行衡量，
这种方法有三个关键的模块，即文本表示、视频表示和公共空间，这三个模块的好坏与检索算法的效果有直接的关系，本文将关注对文本表示和公共空间这两个模块的改进，而视频表示则采用简单但有效的通过预训练的卷积神经网络(ResNeXt-101, ResNet-152)提取的视觉特征~\cite{li2018renmin,li2019renmin}。

%随着深度学习的提出~\cite{hinton2006a}，
在ImageNet图像识别大赛中~\cite{deng2009imagenet}，Hinton组在文献~\cite{krizhevsky2012imagenet}提出的深度卷积网络Alexnet把图像识别错误率从原来的25.7\%提升到15.3\%，极大地推动了深度学习模型的发展，而后一系列的网络层数更深的深度卷积网络VGGNet~\cite{simonyan2014very}，
GoogLeNet~\cite{szegedy2015going}，ResNet~\cite{he2016deep}将图像识别错误率逐步提升到3.57\%，超越了人类的图像识别错误率。
深度学习模型在图像领域的巨大成功，深度学习的方法也被拓展到视频识别领域。考虑到视频比图像而言不仅具有空间信息，也具有时序信息，Simonyan等人在文献~\cite{simonyan2014two}提出双流的深度卷积网络来处理视频的动作识别任务，同时以视频的帧图像和光流信息作为深度卷积网络的输入，证明了深度学习模型在视频动作识别领域比传统的基于人工的特征，如方向梯度直方图（Histogram of Oriented Gradient，HOG）和光流直方图（Histogram of Optical Flow，HOF）的效果相当。因为视频有时序性的特定，后来Du等人在文献~\cite{tran2015learning}上基于图像上的二维卷积核的基础推广到视频上的三维的卷积核，提出C3D深度卷积网络，学习视频的时空特征，提升深度学习方法在视频分类的效果。后来Joao等人在
C3D网络的基础上提出双流的深度卷积网络I3D~\cite{carreira2017quo}，同时使用帧图像和光流作为输入，进一步提升了视频分类的准确率。深度学习模型同样成功地应用在自然语言处理上，例如Tomas提出词向量的分布式表达（Distributed representation）的word2vec模型~\cite{mikolov2013distributed}，使用稠密的词向量表达解决了传统的词向量的one-hot编码的维数灾难问题，而且word2vec的这种表达使得词向量具有词的语义信息，可以直接用来计算不同词之间的相似度。Sepp等人~\cite{hochreiter1997long}提出长短期记忆网络（Long Short-term Memory，LSTM），后来Kyunghyun等人~\cite{cho2014learning}进一步提出门控循环单元（Gated Recurrent Unit，GRU），捕捉序列号的数据的上下文信息。而在视频和自然语言的跨模态领域同样也有大量基于深度学习的工作~\cite{venugopalan2015sequence,yao2015describing,otani2016learning,dong2018predicting,sun2019videobert,miech2019howto100m}，虽然基于深度学习的视频领域有了很大进展~\cite{mithun2018learning,miech2018learning,dong2019dual,liu2019use}，但是这个领域仍然有很大的提升空间。因此本文基于深度学习研究即席视频检索，提高视频检索的效果，具有深远的应用价值和研究价值。


\section{本文研究内容}
本文基于深度学习

本文的贡献可归纳为如下几个方面：
\begin{enumerate}[1.]
\item 技术上，通过对W2VV（原本用于图像-文本匹配）进行改进并再用在即席视频检索任务上，我们提出了一种新的方法W2VV++，有效地提高了即席视频检索的准确率。而对比之前的最好的方法，

\item 2

\end{enumerate}

\section{本文结构安排}
本文基于深度学习对即席视频检索展开研究，共分为五章，具体安排如下：

第一章为绪论，主要介绍基于深度学习的即席视频检索的研究背景与意义，研究内容和主要贡献等。

第二章综述了与本文相关的研究工作，重点介绍了即席视频检索领域的权威评测TRECVID AVS在近几年的发展，也介绍了其他一些跨模态的视频检索的工作，总结他们的优缺点并提出了本文工作的解决方案。

第三章介绍本文的整体算法设计，详细介绍了。。。

第四章介绍了本文实验所用的数据集和算法的实现细节，并且系统地评测了本文算法的性能，并与当前最优的算法做了公平的比较。

第五章对本文工作进行总结，得出结论，并对未来工作进行展望，指出进一步研究的方向。

