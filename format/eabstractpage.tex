% the abstract
\begin{abstracten}
    With the rapid developement of Internet, there is an increasing number of users can readily upload and share a variety of videos through video appication, such as self-created short videos, leading an explosion of video data in the Internet. Therefore, it is of practical value to investigate how to retrieve the video of interest efficiently and effectively. Ad-hoc Video Search (AVS) is to search for unlabled videos relevant with resprect to an ad-hoc query expressed exclusively in terms of a natural-language sentence. It is straight and convenient to express users' need by sentence, and thus it is of great value in practice and of great significance of research. However, the relevance between sentence query and video is difficult to measure because sentence and video are two kinds of media data which are heterogeneous and thus it brings great challenges in video retrieval. To answer these challenges, this paper propose a fully deep learning solution to model sentence query without explicit concept modeling, matching and selecting, which is different from previous concept-based methods. The backbone of the proposed method is W2VV++ model, a super version of Word2VisualVec (W2VV) previously developed for visual-to-text matching. W2VV++ is obtained by tweaking W2VV with a better sentence encoding strategy and an improved triplet ranking loss as loss function. With these simple yet important changes, W2VV++ brings in a substantial improvement. As experiments on MSR-TT and TRECVID AVS show, W2VV++ outperforms the state-of-the-art with mean average precision (mAP) of 20.6\% in MSR-VTT full test dataset and is slightly worse than the stat-of-the-art in TRECVID AVS with an overall inferred average precision of 15.6\%. Considering it is suboptimal to represent sentence query by concatenation of multiple different sentence encoders, this paper goes further and proposes a multi-space multi-loss learning model to ensemble sentence encoders, named sentence encoder assembly (SEA) based on W2VV++. The SEA model learns a separate common space for every sentence encoder independently in a unified framework and the final text-to-video similarity is computed by averaging the similarities over these separate common spaces. The SEA model brings in a great improvement over W2VV++ with mAP of 22.1\% on MSR-VTT full test dataset and overall infAP of 16.8\% on TRECVID AVS and outperforms the state-of-the-art. With SEA, this paper establish a new baseline for ad-hoc video search.

\end{abstracten}
