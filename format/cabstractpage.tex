% the abstract
\begin{abstractzh}
    随着互联网行业的高速发展，越来越多的用户可以很方便地通过视频应用上传分享各式各样的视频，如自行创建的短视频等，互联网上的视频数据在迅猛地增长。因此如何从海量的视频数据里快速、准确地检索到用户需要的视频是个具有很大应用价值的问题。
    本文研究的即席视频检索是指用户以句子的形式表达其需求从未经标注的视频数据里检索到其需要或感兴趣的视频，以句子的方式表达用户需求是种直接方便的方式，因此具有很大的应用价值和研究意义，但是由于句子与视频是两种不同的媒体数据，底层表示上具有异构性，句子与视频的相关性很难被度量，因此这具有很大的挑战。为了应对这些挑战，与之前众多基于概念建模的方法，本文提出了一种完全基于深度学习的方法对查询句子的表达进行建模，这种方法不需要显式地对概念建模、匹配和选择。本文提出的模型以一个原本用于图文匹配的模型Word2VisualVec(W2VV)为基础，称为W2VV++模型。W2VV++模型相对W2VV模型使用了一个更好的文本编码策略并使用了改进的三元组排序损失作为目标函数来监督模型的训练，经过这些简单但有效的改进，W2VV++模型的视频检索效果得到了很大的提高。本文通过实验证明，在MSR-VTT完整的测试集中，W2VV++以平均精度均值（mAP）为20.6\%超过了目前公开最好的算法，在TRECVID的即席视频检索任务（AVS）上也以推测平均精度（infAP）为15.6\%略低于目前公开的最好算法。考虑到W2VV++使用了多个文本编码器进行拼接来作为查询句子的表示不是一种最优的方式，本文在W2VV++基础上进一步提出多空间多目标函数学习的句子编码器融合模型SEA，该模型通过在统一的模型下为每个句子编码器学习独立的公共空间，并根据平均融合这些空间中的查询句子与视频的相似度作为最终的句子与视频的相似度。SEA模型相对W2VV++的视频检索效果进一步得到了提高，在MSR-VTT完整的测试集以mAP为22.1\%和在TRECVID AVS上的infAP为16.8\%的性能都超过了目前公开的最好算法，因此本文提出的算法建立了新的即席视频检索的基准线。

\end{abstractzh}
